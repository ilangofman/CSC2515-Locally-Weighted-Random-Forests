{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from load_data import load_heart_data, load_tweet_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = load_heart_data(\"heart_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train X:  (820, 13)\n",
      "Shape of Test X:  (205, 13)\n",
      "Shape of Train y:  (820,)\n",
      "Shape of Test y:  (205,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Train X: \", train_X.shape)\n",
    "print(\"Shape of Test X: \", test_X.shape)\n",
    "print(\"Shape of Train y: \", train_y.shape)\n",
    "print(\"Shape of Test y: \", test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Performance\n",
    "\n",
    "Before testing out the new method explored in the project regarding the locally weighted trees, we will first test out the existing method we aim to improve upon, the random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the best random forrest model, we need to tune the hyperparameters. Will do \n",
    "# so by performing a grid search \n",
    "\n",
    "grid_search_parameters = {'max_depth':[5, 10, 15, 30, 50, None], \n",
    "                          'n_estimators':[10, 25, 50, 100, 250, 500, 1000], \n",
    "                          'max_samples': [0.3, 0.5, 0.7, 0.8, 0.9, 1]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_classifier = RandomForestClassifier(random_state=0)\n",
    "random_forest_classifier_GS = GridSearchCV(random_forest_classifier, grid_search_parameters, cv=10, verbose=0)\n",
    "random_forest_classifier_GS.fit(train_X, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'max_depth': 10, 'max_samples': 0.9, 'n_estimators': 50}\n",
      "Best Score:  0.9902439024390244\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters: \", random_forest_classifier_GS.best_params_)\n",
    "print(\"Best Score: \", random_forest_classifier_GS.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    1.00000   0.97222   0.98592       108\n",
      "           1    0.97000   1.00000   0.98477        97\n",
      "\n",
      "    accuracy                        0.98537       205\n",
      "   macro avg    0.98500   0.98611   0.98534       205\n",
      "weighted avg    0.98580   0.98537   0.98537       205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the best parameters:\n",
    "random_forest_classifier = RandomForestClassifier(random_state=0, n_estimators=50, max_samples=0.9, max_depth=10)\n",
    "random_forest_classifier.fit(train_X, train_y)\n",
    "predictions = random_forest_classifier.predict(test_X)\n",
    "\n",
    "print(classification_report(test_y, predictions, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locally Weighted Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocallyWeightedRandomForest:\n",
    "    \n",
    "    '''\n",
    "    Constructor for the model class\n",
    "    Input: \n",
    "        n_estimators - number of estimators in the ensemble\n",
    "        criterion - splitting criteria when training the individual trees\n",
    "        max_depth - the max depth for each individual tree\n",
    "        max_samples - the portion of the dataset subsampled for each tree. \n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 n_estimators=100, \n",
    "                 criterion=\"gini\", \n",
    "                 max_depth=None, \n",
    "                 max_samples=None):\n",
    "\n",
    "        self.n_estimators = n_estimators\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        self.max_samples = max_samples\n",
    "        if self.max_samples is None:\n",
    "            self.max_samples = 1.0\n",
    "\n",
    "    '''\n",
    "    Fit the dataset to an ensemble of decision trees. Each decision tree\n",
    "    is trained on a subsample of the dataset determined by the \"max_samples\" values\n",
    "    of the model. \n",
    "\n",
    "    Input: X - the dataset feature values\n",
    "           y - the target values corresponding to the dataset\n",
    "    '''\n",
    "    def fit(self, X, y):\n",
    "        self.estimators_ = []\n",
    "        self.estimator_datasets = {}\n",
    "\n",
    "        total_samples = y.shape[0]\n",
    "        samples_to_draw = int(total_samples * self.max_samples)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # First we sub-sample the dataset \n",
    "            # TODO should we sample with or without replacement for each individual tree itself\n",
    "            sampled_X, sampled_y = resample(X, y, n_samples=samples_to_draw)\n",
    "      \n",
    "\n",
    "            _decision_tree = DecisionTreeClassifier(max_depth=self.max_depth, criterion=self.criterion)\n",
    "            _decision_tree.fit(sampled_X, sampled_y)\n",
    "            self.estimators_.append(_decision_tree)\n",
    "            self.estimator_datasets[_decision_tree] = (sampled_X, sampled_y)\n",
    "    \n",
    "\n",
    "    '''\n",
    "    Calculate the predictions given the distance function and the temperature value for \n",
    "    aggregating the distance values\n",
    "\n",
    "    Input: test_X - the data to calculate the predictions with \n",
    "           distance_function - a function that takes in two parameters \n",
    "                * point - a single point to predict on\n",
    "                * X - the dataset used to train the classifier\n",
    "                This function is meant to allow for a flexible calculation of distances which will get aggregated afterwards\n",
    "            temperature - input to the distance softmax calculation\n",
    "\n",
    "    Output: predictions numpy array \n",
    "    '''\n",
    "    def predict(self, test_X, distance_function = lambda point, x: 1, temperature=1.0):\n",
    "        predictions = np.zeros(test_X.shape[0])\n",
    "        \n",
    "        for index, test_point in enumerate(test_X):\n",
    "            estimator_predictions = {}\n",
    "            estimator_distances = np.zeros(self.n_estimators)\n",
    "\n",
    "            # First loop through all the estimators and calculate the distances\n",
    "            # using the distance functions provided\n",
    "            for i, _estimator in enumerate(self.estimators_):\n",
    "                sampled_dataset = self.estimator_datasets[_estimator]\n",
    "                sampled_X = sampled_dataset[0]\n",
    "                estimator_distances[i] = distance_function(test_point, sampled_X)\n",
    "            \n",
    "            # Calculate the weights. Now all the weights should add to 1. \n",
    "            prediction_weights = self.calculate_weights(estimator_distances, temperature)\n",
    "\n",
    "            # Predict the value using the estimators and the associated weights. \n",
    "            for i, _estimator in enumerate(self.estimators_):\n",
    "                # Make the prediction \n",
    "                est_prediction = _estimator.predict([test_point])[0]\n",
    "                \n",
    "                # If this class hasn't been predicted before, initialize the sum as 0. \n",
    "                if est_prediction not in estimator_predictions:\n",
    "                    estimator_predictions[est_prediction] = 0\n",
    "\n",
    "                # Add the weight of that prediction to the predicted class' running total\n",
    "                estimator_predictions[est_prediction] += prediction_weights[i] \n",
    "\n",
    "            \n",
    "            # The final prediction will be the class with the largest sum of its weights\n",
    "            # Get the argmax of the dictionary. I.e. key with the largest value\n",
    "            predictions[index]  = max(estimator_predictions, key=estimator_predictions.get)\n",
    "\n",
    "        return predictions\n",
    "        \n",
    "\n",
    "    '''\n",
    "    Calculate the weights of the trees using the distances.\n",
    "    The weights are the softmax output of the distances. \n",
    "\n",
    "    Input: - estimator_distances: list of the distances of the point to each tree in the ensemble\n",
    "           - temperature - hyperparameter for the softmax function. \n",
    "    \n",
    "    Output: List of the weight values, the sum should be equal to 1. \n",
    "\n",
    "    '''\n",
    "    def calculate_weights(self, estimator_distances, temperature):\n",
    "        weights = np.zeros(self.n_estimators)\n",
    "        \n",
    "        # TODO Figure out, should it be distance or -1 * distance because closer distances should get a larger value?\n",
    "\n",
    "        # Calculate Denominator values\n",
    "        total_den_sum = 0\n",
    "        for distance in estimator_distances:\n",
    "            total_den_sum += np.exp(-distance / (2 * temperature ** 2))\n",
    "\n",
    "        for i, distance in enumerate(estimator_distances):\n",
    "            weights[i] = np.exp(-distance / (2 * temperature ** 2)) / total_den_sum\n",
    "        \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lwrf = LocallyWeightedRandomForest(n_estimators=50, max_samples=0.9, max_depth=10)\n",
    "lwrf = LocallyWeightedRandomForest(n_estimators=1000, max_samples=0.5, max_depth=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_euclidean_distance(point, dataset):\n",
    "    '''\n",
    "    Compute L2 distance between test point and each training point\n",
    "    \n",
    "    Input: point is a 1d numpy array\n",
    "    Output: dist is a numpy array containing the distances between the test point and each training point\n",
    "    '''\n",
    "    # Source: CSC2515 Homework 3 Starter Code. \n",
    "    \n",
    "    dataset_norm = (dataset**2).sum(axis=1).reshape(-1,1)\n",
    "\n",
    "    # Process test point shape\n",
    "    point = np.squeeze(point)\n",
    "    if point.ndim == 1:\n",
    "        point = point.reshape(1, -1)\n",
    "    assert point.shape[1] == dataset.shape[1]\n",
    "\n",
    "    # Compute squared distance\n",
    "    test_norm = (point**2).sum(axis=1).reshape(1,-1)\n",
    "    dist = dataset_norm + test_norm - 2*dataset.dot(point.transpose())\n",
    "\n",
    "    return np.mean(np.squeeze(dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "lwrf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.98039   0.92593   0.95238       108\n",
      "           1    0.92233   0.97938   0.95000        97\n",
      "\n",
      "    accuracy                        0.95122       205\n",
      "   macro avg    0.95136   0.95265   0.95119       205\n",
      "weighted avg    0.95292   0.95122   0.95125       205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = lwrf.predict(test_X, average_euclidean_distance, temperature=2)\n",
    "print(classification_report(test_y, pred, digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
